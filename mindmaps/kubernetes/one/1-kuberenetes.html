
In the previous lesson, I mentioned the Kubernetes control plane, which is the fleet of cooperating processes that make a Kubernetes cluster work.

Even though you'll only work directly on a few of these components, it helps to know about them and the role each plays.

I'll build up a Kubernetes cluster part by part, explaining each piece as I go.

After I'm done, I'll show you how a Kubernetes cluster running in GKE is a lot less work to manage than one you provision yourself.

Okay, here we go!

First and foremost, your cluster needs computers.

Nowadays, the computers to compose your clusters usually are virtual machines.

They always are in GKE, but they could be physical computers too.

One computer is called the "control plane" and the others are simply called "nodes".

The job of the nodes is to run parts.

The job of the control plane is to coordinate the entire cluster.

We'll meet its control plane components first.

Several critical Kubernetes components run under control plane.

The single component that you interact with directly is called a kube-apiserver.

This component's job is to accept commands that view or change the state of the cluster, including launching pods.

In this specialization, you'll use the kubectl command frequently.

This command's job is to connect to the kube-apiserver and communicate with it using the Kubernetes API.

kube-apiserver also authenticates incoming requests, determines whether they are authorized and valid, and manages admission control.

But it's not just a kubectl that talks with kube-apiserver.

In fact any query or change towards the cluster's state must be addressed to the kube-apiserver.

etcd is the cluster's database.

Its job is to reliably store the state of the cluster.

This includes all of the cluster configuration data and more dynamic information, such as what nodes

are part of the cluster, what pods should be running and where they should be running.

You'll never interact directly with etcd, instead kube-apiserver interacts with the database on behalf of the rest of the system.

kube-scheduler is responsible for scheduling pods onto nodes.

To do that, it evaluates the requirements of each individual pod and selecting which node is most suitable.

But it doesn't do the work of actually launching the pods on the nodes.

Instead, whenever it discovers a pod object that doesn't yet have an assignment to a node,

it chooses a node and simply writes the name of that node into the pod object.

Another component of the system is responsible then, for launching to the pods and you'll see it very soon.

But how does kube-scheduler decide where to run a pod?

It knows the state of all of the nodes and it will obey constraints that you define on where a pod may run, based on hardware, software, and policy.

For example, you might specify that a certain Pod is only allowed to run on nodes with a certain amount of memory.

You can also define affinity specifications, which caused groups of pods to prefer running on the

same node or anti-affinity specifications, which ensure the pods do not run on the same node.

You'll learn more about some of these tools in later modules.

kube-controller-manager has a broader job.

It continuously monitors the state of the cluster through kube-apiserver.

Whenever the current state of the cluster doesn't match the desired state, kube-controller-manager will attempt to make changes to achieve the desired state.

It's called the controller manager because many Kubernetes objects are managed by loops of code called controllers.

These loops of code handle the process of remediation.

Controllers will be very useful for you.

To be specific, you'll learn to use certain kinds of Kubernetes controllers to manage workloads.

For example, remember our problem of keeping 3 nginx pods always running?

We can gather them together into a controller object called a deployment.

That not only keeps them running, but also lets us scale them and bring together under the front end.

We'll meet deployments later in this module.

Other kinds of controllers have system-level responsibilities.

For example, Node Controller's job is to monitor and respond when a node is offline.

kube-cloud-manager manages controllers that interact with the underlying cloud providers.

For example, if you manually launched a Kubernetes cluster on Google Compute Engine, kube-cloud-manager would be

responsible for bringing in Google Cloud features like load balancers and storage volumes when you needed them.

Each node runs a small family of control plane components too.

For example, each node runs a kubelet.

You can think of a kubelet as Kubernetes agent on each node.

When the kube-apiserver wants to start a pod on a node, it connects to that nodes kubelet.

Kubelet uses the container runtime to start the pod and monitors its life cycle, including readiness and liveliness probes and reports back to kube-apiserver.

Do you remember our use of the term "container runtime" in the previous module?

This is the software that knows how to launch a container from a container image.

The world of Kubernetes offers several choices for container runtimes.

But the Linux distribution that GKE uses for its node launches containers using containerd, the runtime component of Docker.

kube-proxy's job is to maintain the network connectivity among the pods in a cluster.

In open-source Kubernetes, it does so using the firewall capabilities of iptables, which are built into the Linux kernel.

Later in the specialization, we will learn how GKE handles pod networking.
